{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import BertModel, BertTokenizer,BertForMaskedLM\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-18 19:13:27,083] [    INFO]\u001b[0m - Already cached /home/ricardo/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\u001b[0m\n",
      "\u001b[32m[2022-05-18 19:14:19,257] [    INFO]\u001b[0m - Weights from pretrained model not used in BertModel: ['cls.predictions.decoder_weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.weight', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight', 'cls.predictions.layer_norm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[32m[2022-05-18 19:14:20,148] [    INFO]\u001b[0m - Already cached /home/ricardo/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str1 = '恒大地产2012年拟裁员30%：1月4日，恒大地产（3333.HK）的一位副总裁向记者确认，公司内部发文，2012年将裁员30%。知情人士透露，按步骤初期只裁10%，因为2009年裁员的时候，矛盾冲突比较激烈。该副总称裁员的原因主要是项目销售问题。'\n",
    "model_inputs1 = tokenizer(test_str1)\n",
    "model_outputs1 = model(**{k:paddle.to_tensor([v]) for (k,v) in model_inputs1.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2608, 1920, 1765, 772, 8151, 2399, 2877, 6161, 1447, 8114, 110, 8038, 122, 3299, 125, 3189, 8024, 2608, 1920, 1765, 772, 8020, 10745, 8152, 119, 100, 8021, 4638, 671, 855, 1199, 2600, 6161, 1403, 6381, 5442, 4802, 6371, 8024, 1062, 1385, 1079, 6956, 1355, 3152, 8024, 8151, 2399, 2199, 6161, 1447, 8114, 110, 511, 4761, 2658, 782, 1894, 6851, 7463, 8024, 2902, 3635, 7752, 1159, 3309, 1372, 6161, 8108, 110, 8024, 1728, 711, 8170, 2399, 6161, 1447, 4638, 3198, 952, 8024, 4757, 4688, 1103, 4960, 3683, 6772, 4080, 4164, 511, 6421, 1199, 2600, 4917, 6161, 1447, 4638, 1333, 1728, 712, 6206, 3221, 7555, 4680, 7218, 1545, 7309, 7579, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['恒', '大', '地', '产', '（', '333', '##3', '.', '[UNK]', '）']\n",
      "['项', '目']\n",
      "['销', '售', '问', '题']\n",
      "['地', '产']\n"
     ]
    }
   ],
   "source": [
    "pos_list1 = [\n",
    "    [18,28],\n",
    "    [103,105],\n",
    "    [105,109],\n",
    "    [20,22],\n",
    "]\n",
    "for pos in pos_list1:\n",
    "    print(tokenizer.convert_ids_to_tokens(model_inputs1['input_ids'][pos[0]:pos[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str2 = '从2021年9月份开始，恒大全国各地项目接连停工率，交楼量不足1万套导致恒大的销量大幅下降；每天都有商票逾期，债券经被停止融资融业务，裁员60%'\n",
    "model_inputs2 = tokenizer(test_str2)\n",
    "model_outputs2 = model(**{k:paddle.to_tensor([v]) for (k,v) in model_inputs2.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 70, 768], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [[[ 0.81039554,  0.15714262, -0.33748814, ..., -0.82522547,\n",
      "           0.03717040,  0.39957264],\n",
      "         [ 0.18499802,  0.41343406,  1.15910769, ..., -0.65995526,\n",
      "          -0.32494196,  0.22273116],\n",
      "         [ 0.66050130,  0.17841768,  1.48442113, ...,  0.11918050,\n",
      "           0.87924564, -0.08119622],\n",
      "         ...,\n",
      "         [-0.75124550, -0.86865503, -0.64447254, ..., -0.53708994,\n",
      "          -0.03471069,  0.24607730],\n",
      "         [ 1.64574575, -0.56429189,  1.26031041, ..., -0.29384169,\n",
      "          -0.11994778,  0.50716025],\n",
      "         [ 0.45596373,  0.08586539,  1.00020325, ..., -0.57331502,\n",
      "          -0.22262847, -0.14491002]]])\n"
     ]
    }
   ],
   "source": [
    "print(model_outputs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['恒', '大', '的', '销', '量', '大', '幅', '下', '降']\n",
      "['恒', '大']\n",
      "['接', '连', '停', '工']\n",
      "['恒', '大']\n"
     ]
    }
   ],
   "source": [
    "pos_list2 = [\n",
    "    [34,43],\n",
    "    [10,12],\n",
    "    [18,22],\n",
    "    [10,12],\n",
    "]\n",
    "for pos in pos_list2:\n",
    "    print(tokenizer.convert_ids_to_tokens(model_inputs2['input_ids'][pos[0]:pos[1]]))\n",
    "\n",
    "entity_embs2 = []\n",
    "for pos in pos_list2:\n",
    "    temp_maxtrix = paddle.stack([model_outputs2[0][0,idx] for idx in range(pos[0],pos[1])])\n",
    "    entity_embs2.append(paddle.max(temp_maxtrix,axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],\n",
    "                      [0.1, 0.2, 0.6, 0.7]],\n",
    "                     dtype='float64', stop_gradient=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[4], dtype=float64, place=CPUPlace, stop_gradient=False,\n",
      "       [0.20000000, 0.30000000, 0.60000000, 0.90000000])\n"
     ]
    }
   ],
   "source": [
    "print(paddle.max(x,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "077e27cbed4bc4ef53cb87468badd11a7a60a73d7494d4644aa5441d1d891a61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PaddleEE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
