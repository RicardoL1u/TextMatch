{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/miniconda3/envs/PaddleEE/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from paddlenlp.trainer import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-23 23:41:41,142] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-1.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-05-23 23:41:41,144] [    INFO]\u001b[0m - Already cached /home/ricardo/.paddlenlp/models/ernie-1.0-base-zh/ernie_v1_chn_base.pdparams\u001b[0m\n",
      "\u001b[32m[2022-05-23 23:41:52,325] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-1.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-05-23 23:41:52,327] [    INFO]\u001b[0m - Already cached /home/ricardo/.paddlenlp/models/ernie-1.0-base-zh/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"chnsenticorp\", splits=[\"train\"])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ernie-1.0-base-zh\", num_classes=len(train_dataset.label_list))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ernie-1.0-base-zh\")\n",
    "\n",
    "def convert_example(example, tokenizer):\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=128, pad_to_max_seq_len=True)\n",
    "    encoded_inputs[\"labels\"] = int(example[\"label\"])\n",
    "    return encoded_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(partial(convert_example, tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-05-23 23:44:10,514] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "device=gpu,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_export=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "learning_rate=5e-05,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output/runs/May23_23-44-10_V_YTAOLIU-NB2,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "minimum_eval_times=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW,\n",
      "output_dir=./output,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./output,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_loss=32768,\n",
      "seed=42,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments('./output')\n",
    "training_args.do_train = True\n",
    "training_args.evaluation_strategy = 'steps'\n",
    "# print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paddle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ricardo/TextMatch/finetune.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ricardo/TextMatch/finetune.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustom_MSE\u001b[39;00m(paddle\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLayer):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ricardo/TextMatch/finetune.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ricardo/TextMatch/finetune.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m(Custom_MSE, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m();\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paddle' is not defined"
     ]
    }
   ],
   "source": [
    "class Custom_MSE(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Custom_MSE, self).__init__();\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        square_difference = paddle.square(predictions - target)\n",
    "        loss_value = paddle.mean(square_difference)\n",
    "        return loss_value\n",
    "    \n",
    "    # def __call__(self, predictions, target):\n",
    "    #   square_difference = torch.square(y_predictions - target)\n",
    "    #   loss_value = torch.mean(square_difference)\n",
    "    #   return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     criterion=paddle.nn.loss.CrossEntropyLoss(),\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset if training_args.do_train else None,\n",
    "#     tokenizer=tokenizer)\n",
    "\n",
    "# if training_args.do_train:\n",
    "#     train_result = trainer.train()\n",
    "#     metrics = train_result.metrics\n",
    "#     trainer.save_model()\n",
    "#     trainer.log_metrics(\"train\", metrics)\n",
    "#     trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "077e27cbed4bc4ef53cb87468badd11a7a60a73d7494d4644aa5441d1d891a61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PaddleEE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
